#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çˆ¬è™«å·¥å…· (Unified Crawler)
==============================
æ•´åˆä¸‰å¤§æ•°æ®æºçš„çˆ¬å–ä¸ä¸‹è½½ï¼š
  1. sse-search   : ä¸Šäº¤æ‰€å…¨ç«™æœç´¢çˆ¬è™«ï¼ˆåŸºäº ES æœç´¢æ¥å£ï¼ŒæŒ‰å…³é”®è¯å›æº¯ï¼‰
  2. sse-inquiry  : ä¸Šäº¤æ‰€é—®è¯¢å‡½ä¸“æ çˆ¬è™«ï¼ˆåŸºäºä¸“æ  APIï¼Œå«é˜²ç›—é“¾ä¸‹è½½ï¼‰
  3. cninfo       : å·¨æ½®èµ„è®¯ç½‘å…¬å‘Šä¸‹è½½å™¨ï¼ˆä» Excel è¯»å–é“¾æ¥æ‰¹é‡ä¸‹è½½ï¼‰
  4. cninfo-search: å·¨æ½®èµ„è®¯ç½‘å…³é”®è¯æ£€ç´¢ï¼ˆå»ºç´¢å¼• + æŒ‰ç´¢å¼•ä¸‹è½½ï¼‰

å®‰è£…ä¾èµ–:
    pip install curl_cffi playwright pandas openpyxl tqdm
    playwright install chromium

ç”¨æ³•:
    # === ä¸Šäº¤æ‰€æœç´¢ (sse-search) ===
    python unified_crawler.py sse-search --keyword <å…³é”®è¯>                   # å…¨è‡ªåŠ¨ï¼šçˆ¬å– -> åˆå¹¶ -> ä¸‹è½½
    python unified_crawler.py sse-search --keyword <å…³é”®è¯> --step crawl      # ä»…çˆ¬å–
    python unified_crawler.py sse-search --keyword <å…³é”®è¯> --step merge      # ä»…åˆå¹¶
    python unified_crawler.py sse-search --keyword <å…³é”®è¯> --step download   # ä»…ä¸‹è½½
    python unified_crawler.py sse-search --keyword å¹´æŠ¥ --output ./data       # è‡ªå®šä¹‰å…³é”®è¯å’Œç›®å½•

    # === ä¸Šäº¤æ‰€é—®è¯¢å‡½ä¸“æ  (sse-inquiry) ===
    python unified_crawler.py sse-inquiry                  # çˆ¬å–å…¨éƒ¨
    python unified_crawler.py sse-inquiry --step test      # æµ‹è¯•è¿é€šæ€§
    python unified_crawler.py sse-inquiry --step download  # ä¸‹è½½æ–‡ä»¶
    python unified_crawler.py sse-inquiry --step verify    # æ ¸å¯¹å¹¶è¡¥å½•
    python unified_crawler.py sse-inquiry --step dedup     # æ–‡ä»¶å»é‡
    python unified_crawler.py sse-inquiry --json xxx.json  # æŒ‡å®š JSON ä¸‹è½½

    # === å·¨æ½®èµ„è®¯ç½‘ (cninfo) ===
    python unified_crawler.py cninfo sample.xlsx                       # ä¸‹è½½å…¨éƒ¨
    python unified_crawler.py cninfo sample.xlsx -o ./å…¬å‘Šæ–‡ä»¶         # æŒ‡å®šç›®å½•
    python unified_crawler.py cninfo sample.xlsx --start 0 --end 10   # æŒ‡å®šèŒƒå›´

    # === å·¨æ½®èµ„è®¯ç½‘ - ä» Excel é“¾æ¥åˆ—ä¸‹è½½ (cninfo-excel) ===
    python unified_crawler.py cninfo-excel links.xlsx -o ./output --col 0

    # === å·¨æ½®èµ„è®¯ç½‘ - å…³é”®è¯æ£€ç´¢å¹¶ä¸‹è½½ (cninfo-search) ===
    python unified_crawler.py cninfo-search <å…³é”®è¯> --step index
    python unified_crawler.py cninfo-search <å…³é”®è¯> --step download
    python unified_crawler.py cninfo-search <å…³é”®è¯> --step all --start-date 2026-01-01 --end-date 2026-02-06
"""

import hashlib
import json
import os
import sys
import re
import time
import glob
import random
import asyncio
import csv
import argparse
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# ---------- ç¬¬ä¸‰æ–¹ä¾èµ–ï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥æ˜¾ç¤ºå‹å¥½é”™è¯¯ï¼‰ ----------
try:
    from curl_cffi import requests as cffi_requests
except ImportError:
    cffi_requests = None

try:
    from playwright.async_api import async_playwright
except ImportError:
    async_playwright = None

try:
    import pandas as pd
except ImportError:
    pd = None

try:
    import requests as std_requests
except ImportError:
    std_requests = None

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False


def _require(lib_obj, name: str, pip_name: str = None):
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å·²å®‰è£…"""
    if lib_obj is None:
        pip_name = pip_name or name
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {name}ï¼Œè¯·è¿è¡Œ: pip install {pip_name}")
        sys.exit(1)


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        å…¨ å±€ é… ç½®                              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# --- ä¸Šäº¤æ‰€æœç´¢ (sse-search) ---
DEFAULT_OUTPUT_ROOT = 'output'
SSE_SEARCH_OUTPUT_DIR = os.path.join(DEFAULT_OUTPUT_ROOT, 'sse_search')
SSE_SEARCH_MERGED_FILE = 'all_merged_results.json'
SSE_SEARCH_MAX_EMPTY_YEARS = 3          # è¿ç»­ N å¹´æ— æ•°æ®åˆ™åœæ­¢å›æº¯

# --- ä¸Šäº¤æ‰€é—®è¯¢å‡½ä¸“æ  (sse-inquiry) ---
SSE_INQUIRY_OUTPUT_DIR = os.path.join(DEFAULT_OUTPUT_ROOT, 'sse_inquiry')
SSE_INQUIRY_PAGE_SIZE = 25

# --- å·¨æ½®èµ„è®¯ç½‘ (cninfo) ---
CNINFO_OUTPUT_DIR = os.path.join(DEFAULT_OUTPUT_ROOT, 'cninfo')

# --- é€šç”¨ ---
MAX_DOWNLOAD_WORKERS = 3                # ä¸‹è½½å¹¶å‘æ•°ï¼ˆè¿‡é«˜å®¹æ˜“è¢«å°ï¼‰


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      å…¬ å…± å·¥ å…· å‡½ æ•°                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_jsonp(text: str) -> dict:
    """è§£æ JSONP å“åº”ï¼Œæå– JSON æ•°æ®"""
    match = re.search(r'jsonpCallback\d+\((.*)\)', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except (json.JSONDecodeError, ValueError):
            pass
    return None


def save_to_csv(data: list, csv_path: str, fieldnames: list):
    """å°†åˆ—è¡¨æ•°æ®ä¿å­˜ä¸º CSV"""
    try:
        with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(data)
        return True
    except Exception as e:
        print(f"    âš ï¸ CSV ä¿å­˜å¤±è´¥: {e}")
        return False


def calculate_md5(filepath: str) -> str:
    """è®¡ç®—æ–‡ä»¶ MD5 å“ˆå¸Œï¼ˆåˆ†å—è¯»å–ï¼Œé˜²æ­¢å¤§æ–‡ä»¶æ’‘çˆ†å†…å­˜ï¼‰"""
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def safe_filename(text: str, max_len: int = 50) -> str:
    """å°†æ–‡æœ¬è½¬ä¸ºå®‰å…¨æ–‡ä»¶å"""
    return re.sub(r'[\\/*?:"<>|\r\n]', '', str(text))[:max_len]


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘          æ¨¡å—ä¸€ï¼šä¸Šäº¤æ‰€å…¨ç«™æœç´¢çˆ¬è™« (SSE Search)                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SSESearchCrawler:
    """
    é€šè¿‡ä¸Šäº¤æ‰€ ES æœç´¢æ¥å£ï¼ŒæŒ‰å…³é”®è¯ + æ—¶é—´æ®µçˆ¬å–æ•°æ®ã€‚
    æ”¯æŒè‡ªåŠ¨æŒ‰å¹´å›æº¯ã€é€’å½’æ‹†åˆ†å¤§æ•°æ®åŒºé—´ã€‚
    æ¥æº: sse_spider.py
    """

    def __init__(self, output_dir: str, keyword: str):
        _require(cffi_requests, 'curl_cffi')
        if not keyword:
            raise ValueError("sse-search å…³é”®è¯ä¸èƒ½ä¸ºç©ºï¼Œè¯·ä½¿ç”¨ --keyword ä¼ å…¥ã€‚")
        self.output_dir = output_dir
        self.keyword = keyword
        self.base_url = "https://query.sse.com.cn/search/getESSearchDoc.do"
        self.site_base = "https://www.sse.com.cn"
        self.session = cffi_requests.Session(impersonate="chrome124")
        self.headers = {
            'Referer': 'https://www.sse.com.cn/home/search/',
            'Origin': 'https://www.sse.com.cn',
            'Accept': '*/*',
        }
        self.session.headers.update(self.headers)
        os.makedirs(self.output_dir, exist_ok=True)
        self._init_session()

    def _init_session(self):
        print(">>> [SSEæœç´¢] åˆå§‹åŒ–ä¼šè¯...")
        try:
            self.session.get("https://www.sse.com.cn/home/search/", timeout=15)
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ [SSEæœç´¢] åˆå§‹åŒ–è­¦å‘Š: {e}")

    # ---------- æ—¶é—´æ ¼å¼åŒ– ----------
    @staticmethod
    def format_time(date_str: str, is_end: bool = False) -> str:
        if not date_str:
            return ''
        suffix = " 23:59:59" if is_end else " 00:00:00"
        return f"{date_str}{suffix}" if ' ' not in date_str else date_str

    # ---------- æŸ¥è¯¢æ€»æ•° ----------
    def check_total_count(self, start_date: str, end_date: str) -> int:
        """æŸ¥è¯¢æŸæ—¶é—´æ®µçš„æ•°æ®æ€»é‡"""
        params = {
            'jsonCallBack': f'jsonpCallback{random.randint(100000, 999999)}',
            'searchword': '', 'page': 0, 'limit': 1, 'spaceId': 3,
            'searchMode': 'precise', 'keyword': self.keyword, 'siteName': 'sse',
            'keywordPosition': 'title,paper_content',
            'publishTimeStart': self.format_time(start_date),
            'publishTimeEnd': self.format_time(end_date, True),
            'channelId': '10001', '_': int(time.time() * 1000)
        }
        try:
            resp = self.session.get(self.base_url, params=params, timeout=15)
            data = parse_jsonp(resp.text)
            if data and data.get('code') == '0':
                return int(data.get('data', {}).get('totalSize', 0))
        except Exception as e:
            print(f"    [!] æŸ¥è¯¢å¼‚å¸¸: {e}")
        return 0

    # ---------- åˆ†é¡µçˆ¬å– ----------
    def search_all(self, start_date: str, end_date: str) -> list:
        """çˆ¬å–æŒ‡å®šæ—¶é—´æ®µå†…çš„å…¨éƒ¨æœç´¢ç»“æœ"""
        all_results = []
        page = 0  # ä» 0 å¼€å§‹ï¼Œå¦åˆ™ä¼šæ¼æ‰ç¬¬ä¸€é¡µ

        while True:
            params = {
                'jsonCallBack': f'jsonpCallback{random.randint(100000, 999999)}',
                'searchword': '', 'page': page, 'limit': 20, 'spaceId': 3,
                'orderByDirection': 'DESC', 'orderByKey': 'score',
                'searchMode': 'precise', 'keyword': self.keyword, 'siteName': 'sse',
                'keywordPosition': 'title,paper_content',
                'publishTimeStart': self.format_time(start_date),
                'publishTimeEnd': self.format_time(end_date, True),
                'channelId': '10001', '_': int(time.time() * 1000)
            }
            try:
                resp = self.session.get(self.base_url, params=params, timeout=20)
                data = parse_jsonp(resp.text)
                if not data or data.get('code') != '0':
                    break

                k_list = data['data'].get('knowledgeList', [])
                if not k_list:
                    break

                for item in k_list:
                    res = self._parse_item(item)
                    if res:
                        all_results.append(res)

                total_page = data['data'].get('totalPage', 0)
                print(f"    >>> æŠ“å–ä¸­: {start_date}~{end_date} | é¡µç : {page + 1}/{total_page} | å·²æŠ“: {len(all_results)}")

                if page >= total_page - 1:
                    break

                page += 1
                time.sleep(random.uniform(0.5, 1.0))
            except Exception as e:
                print(f"    å¼‚å¸¸é‡è¯•: {e}")
                time.sleep(2)
        return all_results

    # ---------- è§£æå•æ¡è®°å½• ----------
    def _parse_item(self, item: dict) -> dict:
        if not item:
            return None
        extend = {ext['name']: ext.get('value')
                  for ext in (item.get('extend') or []) if ext.get('name')}

        title = re.sub(r'</?em>', '', str(item.get('title') or ''))
        curl = extend.get('CURL', '')
        file_url = (f"{self.site_base}{curl}" if curl and not curl.startswith('http')
                    else curl or item.get('url', ''))
        stock_code = extend.get('ZQDM', 'unknown')
        file_type = extend.get('FILETYPE', 'pdf')
        create_time = item.get('createTime', '')
        date_str = str(create_time)[:10].replace('-', '')
        s_title = safe_filename(title)

        return {
            'title': title, 'url': file_url, 'stock_code': stock_code,
            'stock_name': extend.get('GSJC', ''), 'create_time': create_time,
            'file_type': file_type,
            'local_filename': f"{stock_code}_{date_str}_{s_title}.{file_type}"
        }

    # ---------- é€’å½’æ‹†åˆ†æ—¶é—´æ®µ ----------
    def run_recursive(self, start_date: str, end_date: str) -> int:
        """é€’å½’æ‹†åˆ†æ—¶é—´æ®µï¼Œç›´åˆ°æ•°é‡é€‚åˆçˆ¬å–"""
        total = self.check_total_count(start_date, end_date)
        if total == 0:
            return 0

        # æ•°é‡è¿‡å¤šï¼ŒäºŒåˆ†æ‹†åˆ†
        if total > 4800:
            dt_start = datetime.strptime(start_date, "%Y-%m-%d")
            dt_end = datetime.strptime(end_date, "%Y-%m-%d")
            mid = (dt_start + (dt_end - dt_start) / 2).strftime("%Y-%m-%d")
            if mid == end_date:
                mid = start_date
            next_day = (datetime.strptime(mid, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")
            return self.run_recursive(start_date, mid) + self.run_recursive(next_day, end_date)

        # æ•°é‡åˆé€‚ï¼Œç›´æ¥çˆ¬å–
        print(f"  + åŒºé—´ [{start_date} ~ {end_date}] å‘ç° {total} æ¡æ•°æ®ï¼Œå¼€å§‹ä¸‹è½½åˆ—è¡¨...")
        results = self.search_all(start_date, end_date)

        if results:
            base_name = f"{self.keyword}_{start_date.replace('-', '')}_{end_date.replace('-', '')}"
            fieldnames = ['stock_code', 'stock_name', 'title', 'url',
                          'create_time', 'file_type', 'local_filename']

            # ä¿å­˜ JSON
            json_fpath = os.path.join(self.output_dir, f"{base_name}.json")
            with open(json_fpath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # ä¿å­˜ CSV
            csv_fpath = os.path.join(self.output_dir, f"{base_name}.csv")
            if save_to_csv(results, csv_fpath, fieldnames):
                print(f"    âœ… å·²åŒé‡ä¿å­˜: {base_name}.json & {base_name}.csv")

        return len(results)


# ---------- SSE Search åˆå¹¶ ----------
def sse_search_merge(data_dir: str, keyword: str, merged_filename: str) -> str:
    """åˆå¹¶æ‰€æœ‰ SSE æœç´¢ç»“æœ JSON æ–‡ä»¶ï¼ˆå»é‡ï¼‰"""
    print(f"\n>>> [é˜¶æ®µ2] å¼€å§‹åˆå¹¶æ‰€æœ‰ JSON æ–‡ä»¶...")
    patterns = [os.path.join(data_dir, f'{keyword}*.json')]
    files = []
    for p in patterns:
        files.extend(glob.glob(p))

    target_files = sorted([f for f in files if 'merged' not in f])
    if not target_files:
        print("    æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶ã€‚")
        return None

    all_data = []
    seen = set()
    for jf in target_files:
        try:
            with open(jf, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    if item.get('url') and item['url'] not in seen:
                        seen.add(item['url'])
                        all_data.append(item)
        except Exception:
            pass

    # ä¿å­˜ JSON
    out_path = os.path.join(data_dir, merged_filename)
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    # ä¿å­˜ CSV
    csv_path = out_path.replace('.json', '.csv')
    fieldnames = ['stock_code', 'stock_name', 'title', 'url',
                  'create_time', 'file_type', 'local_filename']
    if save_to_csv(all_data, csv_path, fieldnames):
        print(f"    âœ… CSV å·²ç”Ÿæˆ: {csv_path}")

    print(f"    âœ… åˆå¹¶å®Œæˆï¼æ€»è®¡æœ‰æ•ˆè®°å½•: {len(all_data)} æ¡")
    print(f"    æ±‡æ€»æ–‡ä»¶: {out_path}")
    return out_path


# ---------- SSE Search Playwright ä¸‹è½½ ----------
async def _solve_waf(context, url):
    """è§£ç‘æ•°åçˆ¬ç›¾"""
    page = await context.new_page()
    try:
        await page.goto(url, timeout=15000, wait_until='domcontentloaded')
        await asyncio.sleep(3)
    except Exception:
        pass
    finally:
        await page.close()


async def _playwright_download_file(context, url, path):
    """ä½¿ç”¨ Playwright ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        resp = await context.request.get(url, timeout=20000)
        body = await resp.body()

        is_waf = False
        if b'var arg1=' in body or b'var _0x' in body:
            is_waf = True
        elif body[:4] != b'%PDF' and len(body) < 6000 and b'<html' in body:
            is_waf = True

        if is_waf:
            print(" -> ğŸ›¡ï¸ è§¦å‘åçˆ¬ï¼Œè§£ç›¾ä¸­...", end="")
            await _solve_waf(context, url)
            resp = await context.request.get(url, timeout=20000)
            body = await resp.body()
            if body[:4] == b'%PDF':
                print(" -> âœ… æˆåŠŸ", end=" ")
            else:
                return False, f"è§£ç›¾åä»å¤±è´¥ ({len(body)}B)"

        if body[:4] == b'%PDF' or len(body) > 1000:
            with open(path, 'wb') as f:
                f.write(body)
            return True, f"{len(body)} B"
        return False, f"æ— æ•ˆæ–‡ä»¶ (Head: {body[:10]}...)"
    except Exception as e:
        return False, str(e)[:50]


async def sse_search_download(json_path: str, data_dir: str):
    """ä½¿ç”¨ Playwright å¼•æ“ä¸‹è½½ SSE æœç´¢ç»“æœä¸­çš„æ–‡ä»¶"""
    _require(async_playwright, 'playwright', 'playwright')

    print(f"\n>>> [é˜¶æ®µ3] å¯åŠ¨ä¸‹è½½å¼•æ“ (Playwright)...")
    if not os.path.exists(json_path):
        print("âŒ æ‰¾ä¸åˆ°æ±‡æ€»æ–‡ä»¶")
        return

    with open(json_path, 'r', encoding='utf-8') as f:
        results = json.load(f)

    files_dir = os.path.join(os.path.dirname(json_path), 'files')
    os.makedirs(files_dir, exist_ok=True)

    tasks = []
    for r in results:
        fpath = os.path.join(files_dir, r['local_filename'])
        if os.path.exists(fpath) and os.path.getsize(fpath) > 3000:
            continue
        if r.get('url'):
            tasks.append((r['url'], fpath, r['local_filename']))

    print(f"    å¾…ä¸‹è½½ä»»åŠ¡: {len(tasks)} (æ€»æ•°: {len(results)})")
    if not tasks:
        print("    âœ… æ‰€æœ‰æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½ã€‚")
        return

    failed_list = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            args=['--disable-blink-features=AutomationControlled']
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            extra_http_headers={"Referer": "https://www.sse.com.cn/"}
        )

        # é¢„çƒ­
        pg = await context.new_page()
        try:
            await pg.goto('https://www.sse.com.cn/disclosure/listedinfo/announcement/',
                          wait_until='domcontentloaded')
            await asyncio.sleep(3)
        finally:
            await pg.close()

        for i, (url, path, name) in enumerate(tasks):
            print(f"[{i + 1}/{len(tasks)}] {name}", end=" ")
            ok, msg = await _playwright_download_file(context, url, path)
            print(f"âœ“ {msg}" if ok else f"âœ— {msg}")

            if not ok:
                failed_list.append(f"{name} | {url} | {msg}")

            await asyncio.sleep(3 if not ok else random.uniform(0.5, 1.5))

        await browser.close()

    # å†™å…¥å¤±è´¥æ—¥å¿—
    if failed_list:
        fail_log = os.path.join(data_dir, 'download_failed.txt')
        with open(fail_log, 'w', encoding='utf-8') as f:
            f.write('\n'.join(failed_list))
        print(f"\nâš ï¸ æœ‰ {len(failed_list)} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œè¯¦æƒ…å·²è®°å½•åˆ°: {fail_log}")
    else:
        print("\nâœ… æ‰€æœ‰ä»»åŠ¡ä¸‹è½½å®Œæˆï¼Œæ— å¤±è´¥è®°å½•ã€‚")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘         æ¨¡å—äºŒï¼šä¸Šäº¤æ‰€é—®è¯¢å‡½ä¸“æ çˆ¬è™« (SSE Inquiry)                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SSEInquiriesScraper:
    """
    é€šè¿‡ä¸Šäº¤æ‰€é—®è¯¢å‡½ä¸“æ  API çˆ¬å–æ•°æ®ï¼Œå«å¤šçº¿ç¨‹ä¸‹è½½ã€æ ¸å¯¹è¡¥å½•ã€å»é‡ã€‚
    æ¥æº: sse_inquiries.py
    """

    def __init__(self, output_dir: str, page_size: int = SSE_INQUIRY_PAGE_SIZE):
        _require(cffi_requests, 'curl_cffi')
        self.output_dir = output_dir
        self.files_dir = os.path.join(output_dir, 'files')
        self.base_url = "https://query.sse.com.cn/commonSoaQuery.do"
        self.site_base = "https://www.sse.com.cn"
        self.page_url = "https://www.sse.com.cn/disclosure/credibility/supervision/inquiries/"
        self.page_size = page_size

        self.session = cffi_requests.Session(impersonate="chrome124")
        self.headers = {
            'Referer': self.page_url,
            'Origin': 'https://www.sse.com.cn',
            'Accept': '*/*',
        }
        self.session.headers.update(self.headers)

        os.makedirs(self.output_dir, exist_ok=True)
        self._init_session()

    def _init_session(self):
        print("[SSEä¸“æ ] åˆå§‹åŒ–ä¼šè¯...")
        try:
            self.session.get(self.page_url, timeout=15)
            time.sleep(0.5)
            params = self._build_params(page_no=1)
            resp = self.session.get(self.base_url, params=params, timeout=15)
            if '"result"' in resp.text:
                print("âœ… åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("âš ï¸ åˆå§‹åŒ–å¯èƒ½æœ‰é—®é¢˜")
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ–è­¦å‘Š: {e}")

    def _build_params(self, page_no: int = 1, page_size: int = None,
                      stock_code: str = '', start_date: str = '',
                      end_date: str = '') -> dict:
        page_size = page_size or self.page_size
        return {
            'jsonCallBack': f'jsonpCallback{random.randint(10000000, 99999999)}',
            'isPagination': 'true',
            'pageHelp.pageSize': page_size,
            'pageHelp.pageNo': page_no,
            'pageHelp.beginPage': page_no,
            'pageHelp.cacheSize': 1,
            'pageHelp.endPage': page_no,
            'sqlId': 'BS_KCB_GGLL_NEW',
            'siteId': 28,
            'channelId': '10012,10743,10744',
            'type': '4',           # ä¸»æ¿ä¸º 4ï¼›å…¨éƒ¨æ¿å—ä¸ºç©º
            'stockcode': stock_code,
            'extGGDL': '1',        # é—®è¯¢å‡½ä¸º 1ï¼›å…¨éƒ¨ç±»å‹ä¸ºç©º
            'createTime': start_date,
            'createTimeEnd': end_date,
            'order': 'createTime|desc,stockcode|asc',
            '_': int(time.time() * 1000)
        }

    # ---------- è·å–æ€»æ•° ----------
    def get_total_count(self, stock_code: str = '', start_date: str = '',
                        end_date: str = '') -> tuple:
        params = self._build_params(page_no=1, stock_code=stock_code,
                                    start_date=start_date, end_date=end_date)
        try:
            response = self.session.get(self.base_url, params=params, timeout=15)
            data = parse_jsonp(response.text)
            if data:
                page_help = data.get('pageHelp', {})
                return page_help.get('total', 0), page_help.get('pageCount', 0)
        except Exception as e:
            print(f"è·å–æ€»æ•°å¤±è´¥: {e}")
        return None, None

    # ---------- å…¨é‡çˆ¬å– ----------
    def search_all(self, stock_code: str = '', start_date: str = '',
                   end_date: str = '', max_pages: int = None) -> list:
        all_results = []
        page = 1
        errors = 0

        total, total_pages = self.get_total_count(stock_code, start_date, end_date)
        if total is None:
            print("æ— æ³•è·å–æ€»æ•°")
            return []

        print(f"æ€»è®°å½•: {total}, æ€»é¡µæ•°: {total_pages}")

        while True:
            params = self._build_params(page_no=page, stock_code=stock_code,
                                        start_date=start_date, end_date=end_date)
            try:
                response = self.session.get(self.base_url, params=params, timeout=20)
                data = parse_jsonp(response.text)

                if not data:
                    errors += 1
                    if errors >= 5:
                        break
                    time.sleep(3)
                    continue

                results = data.get('result', [])
                if not results:
                    break

                for item in results:
                    parsed = self._parse_item(item)
                    if parsed:
                        all_results.append(parsed)

                errors = 0
                print(f"ç¬¬ {page}/{total_pages} é¡µ, å·²è·å– {len(all_results)} æ¡")

                if page >= total_pages:
                    print("âœ… å…¨éƒ¨å®Œæˆ")
                    break
                if max_pages and page >= max_pages:
                    break

                page += 1
                time.sleep(random.uniform(0.8, 1.5))

            except Exception as e:
                errors += 1
                print(f"å¼‚å¸¸: {e}")
                if errors >= 5:
                    break
                time.sleep(3)

        return all_results

    # ---------- è§£æå•æ¡ ----------
    def _parse_item(self, item: dict) -> dict:
        if not item:
            return None

        stock_code = item.get('STOCKCODE', item.get('stockcode', ''))
        stock_name = item.get('STOCKNAME', item.get('extGSJC', ''))
        title = item.get('TITLE', item.get('docTitle', ''))
        doc_url = item.get('DOCURL', item.get('docURL', ''))
        create_time = item.get('CREATETIME', item.get('createTime', ''))
        doc_type = item.get('DOCTYPE', item.get('docType', ''))

        if doc_url and not doc_url.startswith('http'):
            if doc_url.startswith('www.'):
                doc_url = f"https://{doc_url}"
            elif doc_url.startswith('/'):
                doc_url = f"{self.site_base}{doc_url}"
            else:
                doc_url = f"{self.site_base}/{doc_url}"

        date_str = str(create_time)[:10].replace('-', '') if create_time else ''
        s_title = safe_filename(title)

        file_ext = 'pdf'
        if doc_url:
            if '.doc' in doc_url.lower():
                file_ext = 'doc'
            elif '.xls' in doc_url.lower():
                file_ext = 'xls'

        url_hash = ""
        if doc_url:
            url_hash = hashlib.md5(doc_url.encode('utf-8')).hexdigest()[:6]

        filename = f"{stock_code}_{date_str}_{s_title}_{url_hash}.{file_ext}"

        return {
            'stock_code': stock_code,
            'stock_name': stock_name,
            'title': title,
            'url': doc_url,
            'create_time': create_time,
            'doc_type': doc_type,
            'local_filename': filename,
        }

    # ---------- ä¿å­˜ç»“æœ ----------
    def save_results(self, results: list) -> str:
        if not results:
            return None

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        fieldnames = ['stock_code', 'stock_name', 'title', 'url',
                      'create_time', 'doc_type', 'local_filename']

        json_path = os.path.join(self.output_dir, f'é—®è¯¢å‡½ä¸“æ _{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        csv_path = os.path.join(self.output_dir, f'é—®è¯¢å‡½ä¸“æ _{timestamp}.csv')
        save_to_csv(results, csv_path, fieldnames)

        latest_path = os.path.join(self.output_dir, 'latest_results.json')
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"ä¿å­˜: {json_path}")
        return json_path

    # ---------- ä¸‹è½½æ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹ï¼Œä¿®å¤é˜²ç›—é“¾ï¼‰ ----------
    def download_from_json(self, json_path: str = None, max_workers: int = MAX_DOWNLOAD_WORKERS):
        if json_path is None:
            json_path = os.path.join(self.output_dir, 'latest_results.json')

        if not os.path.exists(json_path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return

        with open(json_path, 'r', encoding='utf-8') as f:
            results = json.load(f)

        os.makedirs(self.files_dir, exist_ok=True)

        download_list = [(r['url'], r['local_filename']) for r in results if r.get('url')]
        print(f"ä¸‹è½½ {len(download_list)} ä¸ªæ–‡ä»¶åˆ° {self.files_dir}")

        success, skip, fail = 0, 0, 0
        failed_files = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._download_file, url, fn): (url, fn)
                       for url, fn in download_list}
            iterator = (tqdm(as_completed(futures), total=len(futures))
                        if HAS_TQDM else as_completed(futures))

            for future in iterator:
                url, fn = futures[future]
                ok, msg = future.result()
                if ok:
                    if "è·³è¿‡" in msg:
                        skip += 1
                    else:
                        success += 1
                else:
                    fail += 1
                    failed_files.append(f"{fn}: {msg}")
                time.sleep(0.3)

        print(f"å®Œæˆ: æˆåŠŸ {success} è·³è¿‡ {skip} å¤±è´¥ {fail}")

        if failed_files:
            fail_log = os.path.join(self.output_dir, 'download_failed.txt')
            with open(fail_log, 'w', encoding='utf-8') as f:
                f.write('\n'.join(failed_files))
            print(f"å¤±è´¥è®°å½•: {fail_log}")

    def _download_file(self, url: str, filename: str) -> tuple:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶ï¼ˆä¿®å¤ç‰ˆï¼šä¿ç•™åŸ Headersï¼Œå¢åŠ é‡è¯•ä¸è¶…æ—¶å®½å®¹åº¦ï¼‰"""
        filepath = os.path.join(self.files_dir, filename)
        if os.path.exists(filepath):
            if os.path.getsize(filepath) > 1000:
                return (True, "è·³è¿‡")
            else:
                os.remove(filepath)

        max_retries = 3
        headers = {
            'Referer': self.page_url,
            'Origin': 'https://www.sse.com.cn',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,'
                      'image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
        }

        for attempt in range(1, max_retries + 1):
            try:
                download_session = cffi_requests.Session(impersonate="chrome124")

                # é¢„è®¿é—®è·å– Cookie
                try:
                    download_session.get(self.page_url, headers=headers, timeout=30)
                except Exception:
                    pass

                time.sleep(random.uniform(0.5, 1.5))

                # æ­£å¼ä¸‹è½½
                resp = download_session.get(url, headers=headers, timeout=60, allow_redirects=True)
                resp.raise_for_status()

                content_type = resp.headers.get('Content-Type', '')
                if 'text/html' in content_type and len(resp.content) < 10000:
                    raise ValueError(f"è¿”å›HTMLè€Œéæ–‡ä»¶: {content_type}")

                with open(filepath, 'wb') as f:
                    f.write(resp.content)

                if os.path.getsize(filepath) < 1000:
                    os.remove(filepath)
                    raise ValueError("æ–‡ä»¶è¿‡å°")

                return (True, "æˆåŠŸ")

            except Exception as e:
                if attempt == max_retries:
                    return (False, f"é‡è¯•{max_retries}æ¬¡åå¤±è´¥: {str(e)}")
                time.sleep(2 * attempt)

        return (False, "æœªçŸ¥é”™è¯¯")

    # ---------- ä» Excel é“¾æ¥åˆ—ä¸‹è½½ ----------
    def download_from_excel(self, excel_path: str, max_workers: int = MAX_DOWNLOAD_WORKERS,
                            col: int = 0):
        _require(pd, 'pandas')

        df = pd.read_excel(excel_path, header=None)
        urls = df.iloc[:, col].dropna().astype(str).str.strip().tolist()

        # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
        seen = set()
        uniq = []
        for u in urls:
            if not u.startswith("http"):
                continue
            if u.startswith("http://"):
                u = "https://" + u[len("http://"):]
            if u not in seen:
                seen.add(u)
                uniq.append(u)

        os.makedirs(self.files_dir, exist_ok=True)

        def make_fn(u: str) -> str:
            u0 = u.split("?", 1)[0]
            base = os.path.basename(u0) or "doc.pdf"
            stem, ext = os.path.splitext(base)
            if not ext:
                ext = ".pdf"
            h = hashlib.md5(u.encode("utf-8")).hexdigest()[:10]
            stem = re.sub(r'[\\/*?:"<>|\r\n]+', "_", stem)[:80] or "doc"
            return f"{stem}_{h}{ext}"

        download_list = [(u, make_fn(u)) for u in uniq]
        print(f"ä¸‹è½½ {len(download_list)} ä¸ªæ–‡ä»¶åˆ° {self.files_dir}")

        failed = []
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futs = [ex.submit(self._download_file, u, fn) for u, fn in download_list]
            for fut, (u, fn) in zip(futs, download_list):
                ok, msg = fut.result()
                if not ok:
                    failed.append(f"{fn}\t{u}\t{msg}")
                time.sleep(0.05)

        if failed:
            fail_log = os.path.join(self.output_dir, "download_failed.txt")
            with open(fail_log, "a", encoding="utf-8") as f:
                f.write("\n".join(failed) + "\n")
            print(f"æœ‰å¤±è´¥ï¼Œè§: {fail_log}")

    # ---------- æ ¸å¯¹å¹¶è¡¥å½• ----------
    def verify_and_retry(self, json_path: str = None):
        """æ ¸å¯¹æœ¬åœ°æ–‡ä»¶ï¼Œæ›´æ–°å¤±è´¥è®°å½•ï¼Œå¹¶å°è¯•è¡¥å½•ä¸‹è½½"""
        if json_path is None:
            json_path = os.path.join(self.output_dir, 'latest_results.json')

        if not os.path.exists(json_path):
            print(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {json_path}")
            return

        print("\n=== å¼€å§‹æ ¸å¯¹æœ¬åœ°æ–‡ä»¶å®Œæ•´æ€§ ===")
        with open(json_path, 'r', encoding='utf-8') as f:
            all_records = json.load(f)

        missing_records = []
        valid_count = 0

        for item in all_records:
            if not item.get('url'):
                continue
            filepath = os.path.join(self.files_dir, item['local_filename'])
            if os.path.exists(filepath) and os.path.getsize(filepath) > 1000:
                valid_count += 1
            else:
                missing_records.append(item)

        print(f"ç†è®ºæ€»æ•°: {len(all_records)}")
        print(f"æœ¬åœ°å®å­˜: {valid_count}")
        print(f"ç¼ºå¤±/æŸå: {len(missing_records)}")

        fail_log = os.path.join(self.output_dir, 'download_failed.txt')

        if missing_records:
            print(f"\næ£€æµ‹åˆ° {len(missing_records)} ä¸ªæ–‡ä»¶ç¼ºå¤±ï¼Œæ­£åœ¨æ›´æ–°é”™è¯¯æ—¥å¿—...")
            with open(fail_log, 'w', encoding='utf-8') as f:
                for item in missing_records:
                    f.write(f"{item['local_filename']}: {item['url']}\n")
            print(f"å·²æ›´æ–°: {fail_log}")

            user_input = input(f"\næ˜¯å¦ç«‹å³å°è¯•ä¸‹è½½è¿™ {len(missing_records)} ä¸ªç¼ºå¤±æ–‡ä»¶? (y/n): ")
            if user_input.lower() == 'y':
                print("\n=== å¼€å§‹è¡¥å½•ä¸‹è½½ ===")
                download_list = [(r['url'], r['local_filename']) for r in missing_records]
                success, fail = 0, 0
                use_threads = len(download_list) > 10

                if use_threads:
                    with ThreadPoolExecutor(max_workers=3) as executor:
                        futures = {executor.submit(self._download_file, url, fn): fn
                                   for url, fn in download_list}
                        iterator = (tqdm(as_completed(futures), total=len(futures))
                                    if HAS_TQDM else as_completed(futures))
                        for future in iterator:
                            ok, msg = future.result()
                            if ok:
                                success += 1
                            else:
                                fail += 1
                                print(f"è¡¥å½•å¤±è´¥: {futures[future]} - {msg}")
                else:
                    for url, fn in download_list:
                        print(f"æ­£åœ¨è¡¥å½•: {fn[:30]}...", end="")
                        ok, msg = self._download_file(url, fn)
                        if ok:
                            print(" [æˆåŠŸ]")
                            success += 1
                        else:
                            print(f" [å¤±è´¥: {msg}]")
                            fail += 1

                print(f"\nè¡¥å½•ç»“æŸ: æˆåŠŸ {success}, ä»å¤±è´¥ {fail}")

                if success > 0:
                    self.verify_and_retry(json_path)
        else:
            print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰æ–‡ä»¶å·²å…¨éƒ¨ä¸‹è½½å®Œæˆï¼")
            if os.path.exists(fail_log):
                os.remove(fail_log)
                print("å·²æ¸…é™¤æ—§çš„é”™è¯¯æ—¥å¿—ã€‚")

    # ---------- æ–‡ä»¶å»é‡ ----------
    def deduplicate_files(self):
        """æ ¹æ®æ–‡ä»¶å†…å®¹ (MD5) æ£€æµ‹å¹¶åˆ é™¤é‡å¤æ–‡ä»¶"""
        if not os.path.exists(self.files_dir):
            print("æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œæ— éœ€å»é‡")
            return

        print("\n=== å¼€å§‹æ‰«æé‡å¤æ–‡ä»¶ (åŸºäºå†…å®¹ MD5) ===")
        files = [f for f in os.listdir(self.files_dir)
                 if os.path.isfile(os.path.join(self.files_dir, f))]
        print(f"æ‰«æç›®å½•: {self.files_dir}")
        print(f"æ–‡ä»¶æ€»æ•°: {len(files)}")

        seen_hashes = {}
        duplicates = []

        iterator = tqdm(files, desc="è®¡ç®—å“ˆå¸Œ") if HAS_TQDM else files
        for filename in iterator:
            filepath = os.path.join(self.files_dir, filename)
            if os.path.getsize(filepath) < 100:
                continue
            file_hash = calculate_md5(filepath)
            if file_hash in seen_hashes:
                duplicates.append((filename, seen_hashes[file_hash]))
            else:
                seen_hashes[file_hash] = filename

        if not duplicates:
            print("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶ã€‚")
            return

        print(f"\nå‘ç° {len(duplicates)} ä¸ªé‡å¤æ–‡ä»¶ã€‚")
        print(f"ç¤ºä¾‹: {duplicates[0][0]} == {duplicates[0][1]}")

        confirm = input("æ˜¯å¦ç¡®è®¤åˆ é™¤è¿™äº›é‡å¤æ–‡ä»¶ï¼Ÿ(y/n): ")
        if confirm.lower() == 'y':
            deleted_count = 0
            freed_space = 0
            for dup_name, _ in duplicates:
                dup_path = os.path.join(self.files_dir, dup_name)
                try:
                    size = os.path.getsize(dup_path)
                    os.remove(dup_path)
                    deleted_count += 1
                    freed_space += size
                except Exception as e:
                    print(f"åˆ é™¤å¤±è´¥ {dup_name}: {e}")
            print(f"\næ¸…ç†å®Œæˆ:")
            print(f"- åˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")
            print(f"- é‡Šæ”¾ç©ºé—´ {freed_space / 1024 / 1024:.2f} MB")
        else:
            print("å·²å–æ¶ˆåˆ é™¤ã€‚")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘           æ¨¡å—ä¸‰ï¼šå·¨æ½®èµ„è®¯ç½‘å…¬å‘Šä¸‹è½½å™¨ (cninfo)                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CninfoDownloader:
    """
    ä» Excel æ–‡ä»¶è¯»å–å·¨æ½®èµ„è®¯ç½‘å…¬å‘Šé“¾æ¥ï¼Œæ‰¹é‡ä¸‹è½½ PDFã€‚
    æ¥æº: cninfo_crawler.py
    """

    def __init__(self, output_dir: str = CNINFO_OUTPUT_DIR):
        _require(pd, 'pandas', 'pandas openpyxl')
        _require(std_requests, 'requests')

        from pathlib import Path
        from urllib.parse import urlparse, parse_qs
        self._urlparse = urlparse
        self._parse_qs = parse_qs

        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'http://www.cninfo.com.cn/',
        }

        self.session = std_requests.Session()
        self.session.headers.update(self.headers)

        self.stats = {'success': 0, 'fail': 0, 'skip': 0}
        self.failed_items = []

    def parse_url(self, url):
        """è§£æå…¬å‘Š URLï¼Œæå– announcementId ç­‰å‚æ•°"""
        if pd.isna(url) or not url:
            return None
        try:
            params = self._parse_qs(self._urlparse(str(url).strip()).query)
            return {
                'announcementId': params.get('announcementId', [''])[0],
                'announcementTime': params.get('announcementTime', [''])[0],
                'stockCode': params.get('stockCode', [''])[0],
                'orgId': params.get('orgId', [''])[0],
            }
        except Exception:
            return None

    def download_file(self, url, save_path, max_retries=3):
        """ä¸‹è½½æ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•"""
        for attempt in range(max_retries):
            try:
                time.sleep(random.uniform(0.8, 1.5))
                response = self.session.get(url, timeout=60, allow_redirects=True)

                if response.status_code == 200:
                    content = response.content
                    if content[:4] == b'%PDF' or len(content) > 5000:
                        with open(save_path, 'wb') as f:
                            f.write(content)
                        if os.path.getsize(save_path) > 1024:
                            return True
                        os.remove(save_path)
                elif response.status_code == 404:
                    return False

            except std_requests.exceptions.Timeout:
                print(f"      è¶…æ—¶ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"      é”™è¯¯: {e}")
        return False

    def download_one(self, row, index):
        """ä¸‹è½½å•æ¡å…¬å‘Š"""
        url = row.get('æ ·æœ¬é“¾æ¥') or row.get('InquiryLink')
        if pd.isna(url) or not url:
            self.stats['skip'] += 1
            return

        params = self.parse_url(url)
        if not params or not params['announcementId']:
            print(f"[{index:04d}] âš  è·³è¿‡: æ— æ•ˆURL")
            self.stats['skip'] += 1
            return

        stock_code = str(row.get('Symbol', params['stockCode']))
        short_name = str(row.get('ShortName', ''))
        ann_id = params['announcementId']
        ann_time = params['announcementTime']

        filename = f"{stock_code}_{short_name}_{ann_time.replace('-', '')}_{ann_id}.PDF"
        filename = re.sub(r'[\\/:*?"<>|\s]', '_', filename)
        save_path = self.output_dir / filename

        if save_path.exists() and save_path.stat().st_size > 1024:
            print(f"[{index:04d}] âœ“ å·²å­˜åœ¨: {stock_code} {short_name}")
            self.stats['success'] += 1
            return

        print(f"[{index:04d}] â†“ ä¸‹è½½ä¸­: {stock_code} {short_name}")

        download_urls = [
            f"http://static.cninfo.com.cn/finalpage/{ann_time}/{ann_id}.PDF",
            f"http://www.cninfo.com.cn/new/announcement/download?bulletinId={ann_id}&realTime=true",
            f"https://static.cninfo.com.cn/finalpage/{ann_time}/{ann_id}.PDF",
        ]

        for dl_url in download_urls:
            if self.download_file(dl_url, save_path):
                print(f"[{index:04d}] âœ“ æˆåŠŸ: {filename}")
                self.stats['success'] += 1
                return

        print(f"[{index:04d}] âœ— å¤±è´¥: {stock_code} {short_name}")
        self.stats['fail'] += 1
        self.failed_items.append({
            'index': index,
            'stock_code': stock_code,
            'short_name': short_name,
            'announcement_id': ann_id,
            'url': url
        })

    def run(self, excel_path, start=0, end=None):
        """æ‰¹é‡ä¸‹è½½"""
        print("=" * 60)
        print("  å·¨æ½®èµ„è®¯ç½‘å…¬å‘Šä¸‹è½½å™¨")
        print("=" * 60)
        print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  æ–‡ä»¶: {excel_path}")

        df = pd.read_excel(excel_path, sheet_name=3)
        total = len(df)
        end = end or total

        print(f"  æ€»æ•°: {total} æ¡")
        print(f"  èŒƒå›´: {start} - {end}")
        print(f"  ç›®å½•: {self.output_dir.absolute()}")
        print("-" * 60)

        start_time = time.time()

        for i in range(start, min(end, total)):
            self.download_one(df.iloc[i].to_dict(), i)

            if (i + 1) % 10 == 0:
                elapsed = time.time() - start_time
                progress = (i + 1 - start) / (end - start) * 100
                print(f"\n--- è¿›åº¦: {i + 1}/{end} ({progress:.1f}%) è€—æ—¶: {elapsed:.0f}s ---\n")
                time.sleep(random.uniform(2, 4))

        elapsed = time.time() - start_time
        print("\n" + "=" * 60)
        print("  ä¸‹è½½å®Œæˆ!")
        print("-" * 60)
        print(f"  æˆåŠŸ: {self.stats['success']}")
        print(f"  å¤±è´¥: {self.stats['fail']}")
        print(f"  è·³è¿‡: {self.stats['skip']}")
        print(f"  è€—æ—¶: {elapsed:.1f} ç§’")

        if self.failed_items:
            failed_path = self.output_dir / "ä¸‹è½½å¤±è´¥åˆ—è¡¨.xlsx"
            pd.DataFrame(self.failed_items).to_excel(failed_path, index=False)
            print(f"\n  å¤±è´¥åˆ—è¡¨å·²ä¿å­˜: {failed_path}")

        print("=" * 60)


class CninfoSearchDownloader:
    """
    å·¨æ½®èµ„è®¯ç½‘å…³é”®è¯æ£€ç´¢å™¨ï¼š
    1) æŒ‰å…³é”®è¯ + æ—¥æœŸèŒƒå›´æ£€ç´¢å…¬å‘Šå¹¶å»ºç«‹ç´¢å¼•ï¼ˆJSON/CSVï¼‰
    2) æ ¹æ®ç´¢å¼•ä¸‹è½½é™„ä»¶å¹¶è¾“å‡ºä¸‹è½½æŠ¥å‘Š
    """

    INDEX_FIELDS = [
        'announcement_id',
        'sec_code',
        'sec_name',
        'org_id',
        'announcement_title_raw',
        'announcement_title',
        'announcement_time_ms',
        'announcement_date',
        'adjunct_url',
        'download_url_static',
        'adjunct_type',
        'adjunct_size',
        'keyword',
        'index_created_at',
    ]

    REPORT_FIELDS = [
        'announcement_id',
        'sec_code',
        'sec_name',
        'announcement_date',
        'download_url_static',
        'local_filename',
        'status',
        'error',
        'file_path',
    ]

    def __init__(self, output_dir: str = CNINFO_OUTPUT_DIR):
        _require(pd, 'pandas', 'pandas openpyxl')
        _require(std_requests, 'requests')

        from urllib.parse import urlparse
        self._urlparse = urlparse

        self.base_output = os.path.join(output_dir, 'cninfo_search')
        self.index_dir = os.path.join(self.base_output, 'index')
        self.files_dir = os.path.join(self.base_output, 'files')
        os.makedirs(self.index_dir, exist_ok=True)
        os.makedirs(self.files_dir, exist_ok=True)

        self.search_url = 'https://www.cninfo.com.cn/new/hisAnnouncement/query'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'X-Requested-With': 'XMLHttpRequest',
            'Origin': 'https://www.cninfo.com.cn',
            'Referer': 'https://www.cninfo.com.cn/new/commonUrl/pageOfSearch?url=disclosure/list/search',
        }
        self.session = std_requests.Session()
        self.session.headers.update(self.headers)

    @staticmethod
    def _strip_html(raw_text: str) -> str:
        txt = re.sub(r'<[^>]+>', '', str(raw_text or ''))
        txt = txt.replace('&nbsp;', ' ').replace('&amp;', '&')
        return txt.strip()

    @staticmethod
    def _to_date_text(timestamp_ms) -> str:
        if timestamp_ms is None:
            return ''
        try:
            t = int(float(timestamp_ms))
            if t > 10**11:
                t = t / 1000
            return datetime.fromtimestamp(t).strftime('%Y-%m-%d')
        except Exception:
            return ''

    @staticmethod
    def _normalize_page_size(page_size: int) -> int:
        try:
            page_size = int(page_size)
        except Exception:
            page_size = 30
        return max(1, min(page_size, 30))

    def _build_payload(self, keyword: str, start_date: str, end_date: str,
                       page_no: int, page_size: int) -> dict:
        return {
            'pageNum': str(page_no),
            'pageSize': str(page_size),
            'column': 'szse',
            'tabName': 'fulltext',
            'plate': '',
            'stock': '',
            'searchkey': keyword,
            'secid': '',
            'category': '',
            'trade': '',
            'seDate': f'{start_date}~{end_date}',
            'sortName': '',
            'sortType': '',
            'isHLtitle': 'true',
        }

    def _normalize_record(self, item: dict, keyword: str, index_created_at: str) -> dict:
        announcement_id = str(item.get('announcementId') or '').strip()
        adjunct_url = str(item.get('adjunctUrl') or '').strip()
        if adjunct_url and adjunct_url.startswith('http'):
            download_url_static = adjunct_url
        elif adjunct_url:
            download_url_static = f"https://static.cninfo.com.cn/{adjunct_url.lstrip('/')}"
        else:
            download_url_static = ''

        raw_title = str(item.get('announcementTitle') or '')
        ann_time = item.get('announcementTime')

        return {
            'announcement_id': announcement_id,
            'sec_code': str(item.get('secCode') or '').strip(),
            'sec_name': self._strip_html(item.get('secName') or ''),
            'org_id': str(item.get('orgId') or '').strip(),
            'announcement_title_raw': raw_title,
            'announcement_title': self._strip_html(raw_title),
            'announcement_time_ms': ann_time if ann_time is not None else '',
            'announcement_date': self._to_date_text(ann_time),
            'adjunct_url': adjunct_url,
            'download_url_static': download_url_static,
            'adjunct_type': str(item.get('adjunctType') or '').strip(),
            'adjunct_size': item.get('adjunctSize') if item.get('adjunctSize') is not None else '',
            'keyword': keyword,
            'index_created_at': index_created_at,
        }

    def search_and_build_index(self, keyword: str, start_date: str, end_date: str,
                               page_size: int = 30, max_pages: int = None,
                               max_results: int = None) -> list:
        page_size = self._normalize_page_size(page_size)

        print(f"\n=== [cninfo-search] å¼€å§‹æ£€ç´¢ ===")
        print(f"å…³é”®è¯: {keyword}")
        print(f"èŒƒå›´: {start_date} ~ {end_date}")
        print(f"é¡µå¤§å°: {page_size} (æ¥å£ä¸Šé™ 30)")
        if max_pages:
            print(f"æœ€å¤§é¡µæ•°: {max_pages}")
        if max_results:
            print(f"æœ€å¤§ç»“æœæ•°: {max_results}")

        all_records = []
        seen_keys = set()
        page_no = 1
        created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        while True:
            payload = self._build_payload(keyword, start_date, end_date, page_no, page_size)
            try:
                resp = self.session.post(self.search_url, data=payload, timeout=30)
                resp.raise_for_status()
                data = resp.json()
            except Exception as e:
                print(f"âŒ ç¬¬ {page_no} é¡µè¯·æ±‚å¤±è´¥: {e}")
                break

            announcements = data.get('announcements') or []
            total_announcement = data.get('totalAnnouncement')
            has_more = bool(data.get('hasMore'))

            if not announcements:
                print(f"ç¬¬ {page_no} é¡µæ— æ•°æ®ï¼Œç»“æŸåˆ†é¡µã€‚")
                break

            page_added = 0
            for item in announcements:
                record = self._normalize_record(item, keyword, created_at)
                uniq_key = record.get('announcement_id') or record.get('download_url_static')
                if not uniq_key or uniq_key in seen_keys:
                    continue
                seen_keys.add(uniq_key)
                all_records.append(record)
                page_added += 1
                if max_results and len(all_records) >= max_results:
                    break

            print(f"é¡µ {page_no}: æœ¬é¡µ {len(announcements)} æ¡, æ–°å¢ {page_added} æ¡, "
                  f"ç´¯è®¡ {len(all_records)} æ¡, æ¥å£æ€»é‡ {total_announcement}")

            if max_results and len(all_records) >= max_results:
                print(f"è¾¾åˆ° max-results={max_results}ï¼Œåœæ­¢æŠ“å–ã€‚")
                break
            if max_pages and page_no >= max_pages:
                print(f"è¾¾åˆ° max-pages={max_pages}ï¼Œåœæ­¢æŠ“å–ã€‚")
                break
            if not has_more:
                print("hasMore=Falseï¼Œåœæ­¢æŠ“å–ã€‚")
                break

            page_no += 1
            time.sleep(random.uniform(0.5, 1.0))

        return all_records

    def _keyword_tag(self, keyword: str) -> str:
        tag = safe_filename(keyword, max_len=40).strip().replace(' ', '_')
        return tag or 'keyword'

    def save_index(self, keyword: str, records: list) -> dict:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        key_tag = self._keyword_tag(keyword)

        snapshot_json = os.path.join(self.index_dir, f'cninfo_search_{key_tag}_{timestamp}.json')
        snapshot_csv = os.path.join(self.index_dir, f'cninfo_search_{key_tag}_{timestamp}.csv')
        latest_json = os.path.join(self.index_dir, 'latest_index.json')
        latest_csv = os.path.join(self.index_dir, 'latest_index.csv')

        with open(snapshot_json, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        with open(latest_json, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)

        save_to_csv(records, snapshot_csv, self.INDEX_FIELDS)
        save_to_csv(records, latest_csv, self.INDEX_FIELDS)

        print("\n=== [cninfo-search] ç´¢å¼•å·²ç”Ÿæˆ ===")
        print(f"è®°å½•æ•°: {len(records)}")
        print(f"å¿«ç…§ JSON: {snapshot_json}")
        print(f"å¿«ç…§ CSV : {snapshot_csv}")
        print(f"æœ€æ–° JSON: {latest_json}")
        print(f"æœ€æ–° CSV : {latest_csv}")

        return {
            'snapshot_json': snapshot_json,
            'snapshot_csv': snapshot_csv,
            'latest_json': latest_json,
            'latest_csv': latest_csv,
        }

    def _load_index(self, index_path: str = None) -> tuple:
        index_path = index_path or os.path.join(self.index_dir, 'latest_index.json')
        if not os.path.exists(index_path):
            print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
            return [], index_path

        records = []
        try:
            if index_path.lower().endswith('.json'):
                with open(index_path, 'r', encoding='utf-8') as f:
                    records = json.load(f)
            elif index_path.lower().endswith('.csv'):
                df = pd.read_csv(index_path, dtype=str).fillna('')
                records = df.to_dict('records')
            else:
                print(f"âŒ ä¸æ”¯æŒçš„ç´¢å¼•æ ¼å¼: {index_path}")
                return [], index_path
        except Exception as e:
            print(f"âŒ è¯»å–ç´¢å¼•å¤±è´¥: {e}")
            return [], index_path

        if not isinstance(records, list):
            records = []
        return records, index_path

    def _guess_ext(self, record: dict) -> str:
        adjunct_url = str(record.get('adjunct_url') or '').strip()
        if adjunct_url:
            ext = os.path.splitext(self._urlparse(adjunct_url).path)[1]
            if ext:
                return ext.lower()

        file_type = re.sub(r'[^a-zA-Z0-9]', '', str(record.get('adjunct_type') or '')).lower()
        if file_type:
            return f".{file_type[:8]}"
        return '.pdf'

    def _build_local_filename(self, record: dict) -> str:
        sec_code = safe_filename(record.get('sec_code') or 'unknown', max_len=20) or 'unknown'
        sec_name = safe_filename(record.get('sec_name') or 'unknown', max_len=40) or 'unknown'
        sec_name = sec_name.replace(' ', '_')
        ann_date = re.sub(r'[^0-9]', '', str(record.get('announcement_date') or ''))
        ann_date = ann_date or 'unknown'
        ann_id = safe_filename(record.get('announcement_id') or 'noid', max_len=40) or 'noid'
        ext = self._guess_ext(record)
        return f"{sec_code}_{sec_name}_{ann_date}_{ann_id}{ext}"

    def _download_file(self, url: str, save_path: str, max_retries: int = 3) -> tuple:
        err = ''
        for attempt in range(max_retries):
            try:
                time.sleep(random.uniform(0.5, 1.2))
                response = self.session.get(url, timeout=60, allow_redirects=True)
                if response.status_code == 200:
                    content = response.content
                    ctype = response.headers.get('Content-Type', '').lower()
                    if 'text/html' in ctype and len(content) < 50000 and b'<html' in content[:2000].lower():
                        err = f"è¿”å› HTML ({ctype})"
                    else:
                        with open(save_path, 'wb') as f:
                            f.write(content)
                        if os.path.getsize(save_path) > 1024:
                            return True, "æˆåŠŸ"
                        os.remove(save_path)
                        err = "æ–‡ä»¶è¿‡å°"
                elif response.status_code in (404, 410):
                    return False, f"HTTP {response.status_code}"
                else:
                    err = f"HTTP {response.status_code}"
            except std_requests.exceptions.Timeout:
                err = f"è¯·æ±‚è¶…æ—¶({attempt + 1}/{max_retries})"
            except Exception as e:
                err = str(e)

            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
        return False, (err or "æœªçŸ¥é”™è¯¯")

    def download_from_index(self, index_path: str = None, max_workers: int = MAX_DOWNLOAD_WORKERS) -> str:
        records, index_path = self._load_index(index_path)
        if not records:
            print("ç´¢å¼•ä¸ºç©ºï¼Œè·³è¿‡ä¸‹è½½ã€‚")
            return None

        max_workers = max(1, int(max_workers or 1))
        report_rows = []
        download_tasks = []
        seen_keys = set()

        for record in records:
            record = record or {}
            key = str(record.get('announcement_id') or record.get('download_url_static') or '').strip()
            if key and key in seen_keys:
                report_rows.append({
                    'announcement_id': str(record.get('announcement_id') or ''),
                    'sec_code': str(record.get('sec_code') or ''),
                    'sec_name': str(record.get('sec_name') or ''),
                    'announcement_date': str(record.get('announcement_date') or ''),
                    'download_url_static': str(record.get('download_url_static') or ''),
                    'local_filename': '',
                    'status': 'skip',
                    'error': 'duplicate_key',
                    'file_path': '',
                })
                continue
            if key:
                seen_keys.add(key)

            url = str(record.get('download_url_static') or record.get('adjunct_url') or '').strip()
            local_filename = self._build_local_filename(record)
            save_path = os.path.join(self.files_dir, local_filename)

            if not url:
                report_rows.append({
                    'announcement_id': str(record.get('announcement_id') or ''),
                    'sec_code': str(record.get('sec_code') or ''),
                    'sec_name': str(record.get('sec_name') or ''),
                    'announcement_date': str(record.get('announcement_date') or ''),
                    'download_url_static': '',
                    'local_filename': local_filename,
                    'status': 'skip',
                    'error': 'missing_url',
                    'file_path': save_path,
                })
                continue

            if os.path.exists(save_path) and os.path.getsize(save_path) > 1024:
                report_rows.append({
                    'announcement_id': str(record.get('announcement_id') or ''),
                    'sec_code': str(record.get('sec_code') or ''),
                    'sec_name': str(record.get('sec_name') or ''),
                    'announcement_date': str(record.get('announcement_date') or ''),
                    'download_url_static': url,
                    'local_filename': local_filename,
                    'status': 'skip',
                    'error': '',
                    'file_path': save_path,
                })
                continue

            download_tasks.append((record, url, local_filename, save_path))

        print("\n=== [cninfo-search] å¼€å§‹æŒ‰ç´¢å¼•ä¸‹è½½ ===")
        print(f"ç´¢å¼•æ–‡ä»¶: {index_path}")
        print(f"æ€»è®°å½•æ•°: {len(records)}")
        print(f"å¾…ä¸‹è½½: {len(download_tasks)}")
        print(f"çº¿ç¨‹æ•°: {max_workers}")

        if download_tasks:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(self._download_file, url, save_path): (record, url, local_filename, save_path)
                    for record, url, local_filename, save_path in download_tasks
                }
                iterator = (tqdm(as_completed(futures), total=len(futures), desc='cninfoä¸‹è½½')
                            if HAS_TQDM else as_completed(futures))

                for future in iterator:
                    record, url, local_filename, save_path = futures[future]
                    ok, msg = future.result()
                    report_rows.append({
                        'announcement_id': str(record.get('announcement_id') or ''),
                        'sec_code': str(record.get('sec_code') or ''),
                        'sec_name': str(record.get('sec_name') or ''),
                        'announcement_date': str(record.get('announcement_date') or ''),
                        'download_url_static': url,
                        'local_filename': local_filename,
                        'status': 'success' if ok else 'fail',
                        'error': '' if ok else msg,
                        'file_path': save_path,
                    })

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = os.path.join(self.base_output, f'download_report_{timestamp}.csv')
        save_to_csv(report_rows, report_path, self.REPORT_FIELDS)

        success = sum(1 for r in report_rows if r['status'] == 'success')
        fail = sum(1 for r in report_rows if r['status'] == 'fail')
        skip = sum(1 for r in report_rows if r['status'] == 'skip')

        print("\n=== [cninfo-search] ä¸‹è½½å®Œæˆ ===")
        print(f"æˆåŠŸ: {success}")
        print(f"å¤±è´¥: {fail}")
        print(f"è·³è¿‡: {skip}")
        print(f"æŠ¥å‘Š: {report_path}")
        return report_path

    @staticmethod
    def _parse_date(date_text: str) -> datetime:
        return datetime.strptime(date_text, '%Y-%m-%d')

    def run(self, keyword: str, step: str = 'index', start_date: str = None, end_date: str = None,
            page_size: int = 30, max_pages: int = None, max_results: int = None,
            index_path: str = None, workers: int = MAX_DOWNLOAD_WORKERS):
        if step not in ('index', 'download', 'all'):
            raise ValueError(f"ä¸æ”¯æŒçš„ step: {step}")
        if max_pages is not None and max_pages <= 0:
            raise ValueError("max-pages å¿…é¡»å¤§äº 0")
        if max_results is not None and max_results <= 0:
            raise ValueError("max-results å¿…é¡»å¤§äº 0")
        if workers is not None and workers <= 0:
            raise ValueError("workers å¿…é¡»å¤§äº 0")

        today = datetime.now().strftime('%Y-%m-%d')
        default_start = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
        start_date = start_date or default_start
        end_date = end_date or today

        try:
            st = self._parse_date(start_date)
            ed = self._parse_date(end_date)
        except Exception:
            raise ValueError("æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD")
        if st > ed:
            raise ValueError("start-date ä¸èƒ½æ™šäº end-date")

        generated = None
        if step in ('index', 'all'):
            records = self.search_and_build_index(
                keyword=keyword,
                start_date=start_date,
                end_date=end_date,
                page_size=page_size,
                max_pages=max_pages,
                max_results=max_results,
            )
            generated = self.save_index(keyword, records)

        if step in ('download', 'all'):
            target_index = index_path
            if step == 'all':
                target_index = target_index or (generated.get('latest_json') if generated else None)
            self.download_from_index(target_index, workers)


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                     CLI å‘½ ä»¤ è¡Œ å…¥ å£                             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cmd_sse_search(args):
    """å¤„ç† sse-search å­å‘½ä»¤"""
    data_dir = args.output or SSE_SEARCH_OUTPUT_DIR
    keyword = args.keyword
    if not keyword:
        print("âŒ sse-search éœ€è¦æä¾›å…³é”®è¯ï¼Œè¯·ä½¿ç”¨ --keyword <å…³é”®è¯>")
        return
    merged_file = SSE_SEARCH_MERGED_FILE
    step = args.step or 'all'

    # 1. çˆ¬å–é˜¶æ®µ
    if step in ('all', 'crawl'):
        crawler = SSESearchCrawler(data_dir, keyword=keyword)
        curr_year = datetime.now().year
        year = curr_year
        empty_cnt = 0

        print(f"=== [é˜¶æ®µ1] å¼€å§‹å›æº¯çˆ¬å– (ä» {curr_year} å¼€å§‹) ===")
        print(f"    å…³é”®è¯: {keyword}")
        print(f"    ç­–ç•¥: é‡åˆ°è¿ç»­ {SSE_SEARCH_MAX_EMPTY_YEARS} å¹´æ— æ•°æ®åˆ™åœæ­¢ã€‚\n")

        while True:
            start = f"{year}-01-01"
            end = (f"{year}-12-31" if year != curr_year
                   else datetime.now().strftime("%Y-%m-%d"))

            print(f"--- æ­£åœ¨æ£€æŸ¥ {year} å¹´ ---")
            total_found = crawler.run_recursive(start, end)

            if total_found > 0:
                empty_cnt = 0
                print(f"    {year} å¹´å…±è·å– {total_found} æ¡æ•°æ®ã€‚")
            else:
                print(f"    {year} å¹´æ— æ•°æ®ã€‚")
                empty_cnt += 1

            if empty_cnt >= SSE_SEARCH_MAX_EMPTY_YEARS:
                print(f"\n>>> è¿ç»­ {empty_cnt} å¹´æ— æ•°æ®ï¼Œåˆ¤å®šå·²å›æº¯è‡³å°½å¤´ã€‚åœæ­¢çˆ¬å–ã€‚")
                break

            year -= 1
            time.sleep(1)

    # 2. åˆå¹¶é˜¶æ®µ
    merged_path = os.path.join(data_dir, merged_file)
    if step in ('all', 'merge', 'crawl'):
        res_path = sse_search_merge(data_dir, keyword, merged_file)
        if res_path:
            merged_path = res_path

    # 3. ä¸‹è½½é˜¶æ®µ
    if step in ('all', 'download'):
        asyncio.run(sse_search_download(merged_path, data_dir))


def cmd_sse_inquiry(args):
    """å¤„ç† sse-inquiry å­å‘½ä»¤"""
    output_dir = args.output or SSE_INQUIRY_OUTPUT_DIR
    step = args.step or 'crawl'
    json_path = args.json

    scraper = SSEInquiriesScraper(output_dir=output_dir)

    if step == 'test':
        print("\n=== æµ‹è¯• ===")
        total, pages = scraper.get_total_count()
        print(f"æ€»è®°å½•: {total}, æ€»é¡µæ•°: {pages}")
        if total:
            results = scraper.search_all(max_pages=1)
            print(f"è·å–: {len(results)} æ¡")
            if results:
                print(f"ç¤ºä¾‹: [{results[0]['stock_code']}] {results[0]['title'][:40]}...")
                print("\næµ‹è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶...")
                scraper.files_dir = os.path.join(scraper.output_dir, 'test_download')
                os.makedirs(scraper.files_dir, exist_ok=True)
                ok, msg = scraper._download_file(results[0]['url'], results[0]['local_filename'])
                print(f"âœ… ä¸‹è½½æˆåŠŸ: {results[0]['local_filename']}" if ok
                      else f"âŒ ä¸‹è½½å¤±è´¥: {msg}")

    elif step == 'crawl':
        print("\n=== å¼€å§‹çˆ¬å–é—®è¯¢å‡½ä¸“æ  ===\n")
        results = scraper.search_all()
        if results:
            scraper.save_results(results)
            print(f"\nå…±è·å– {len(results)} æ¡è®°å½•")

    elif step == 'download':
        scraper.download_from_json(json_path, MAX_DOWNLOAD_WORKERS)

    elif step == 'verify':
        scraper.verify_and_retry(json_path)

    elif step == 'dedup':
        scraper.deduplicate_files()

    elif step == 'download-excel':
        if not json_path:
            print("âŒ éœ€è¦æŒ‡å®š Excel æ–‡ä»¶è·¯å¾„ï¼Œç”¨ --json å‚æ•°")
            return
        col = args.col if hasattr(args, 'col') else 0
        scraper.download_from_excel(json_path, MAX_DOWNLOAD_WORKERS, col)


def cmd_cninfo(args):
    """å¤„ç† cninfo å­å‘½ä»¤"""
    excel_path = args.excel_file
    if not os.path.exists(excel_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
        return

    output_dir = args.output or CNINFO_OUTPUT_DIR
    downloader = CninfoDownloader(output_dir=output_dir)
    downloader.run(excel_path, args.start, args.end)


def cmd_cninfo_search(args):
    """å¤„ç† cninfo-search å­å‘½ä»¤ï¼šå…³é”®è¯æ£€ç´¢ -> å»ºç´¢å¼• -> ä¸‹è½½"""
    output_dir = args.output or CNINFO_OUTPUT_DIR
    crawler = CninfoSearchDownloader(output_dir=output_dir)

    try:
        crawler.run(
            keyword=args.keyword,
            step=args.step,
            start_date=args.start_date,
            end_date=args.end_date,
            page_size=args.page_size,
            max_pages=args.max_pages,
            max_results=args.max_results,
            index_path=args.index,
            workers=args.workers,
        )
    except ValueError as e:
        print(f"âŒ å‚æ•°é”™è¯¯: {e}")


def cmd_cninfo_excel(args):
    """å¤„ç† cninfo-excel å­å‘½ä»¤ï¼šä» Excel é“¾æ¥åˆ—ç›´æ¥ä¸‹è½½"""
    excel_path = args.excel_file
    if not os.path.exists(excel_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
        return

    output_dir = args.output or SSE_INQUIRY_OUTPUT_DIR
    scraper = SSEInquiriesScraper(output_dir=output_dir)
    scraper.download_from_excel(excel_path, MAX_DOWNLOAD_WORKERS, args.col)


def main():
    parser = argparse.ArgumentParser(
        description='ç»Ÿä¸€çˆ¬è™«å·¥å…· â€” æ•´åˆä¸Šäº¤æ‰€æœç´¢ã€ä¸Šäº¤æ‰€ä¸“æ ã€å·¨æ½®èµ„è®¯ç½‘',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  python unified_crawler.py sse-search --keyword é—®è¯¢å‡½         # ä¸Šäº¤æ‰€æœç´¢å…¨è‡ªåŠ¨
  python unified_crawler.py sse-search --keyword å¹´æŠ¥ --step crawl  # ä»…çˆ¬å–
  python unified_crawler.py sse-inquiry                         # é—®è¯¢å‡½ä¸“æ å…¨é‡çˆ¬å–
  python unified_crawler.py sse-inquiry --step download         # ä»…ä¸‹è½½
  python unified_crawler.py cninfo sample.xlsx -o ./pdfs        # å·¨æ½®æ‰¹é‡ä¸‹è½½
  python unified_crawler.py cninfo-search é—®è¯¢å‡½ --step index   # å·¨æ½®å…³é”®è¯æ£€ç´¢å¹¶å»ºç´¢å¼•
  python unified_crawler.py cninfo-search å¹´æŠ¥ --step all       # æ£€ç´¢å¹¶æŒ‰ç´¢å¼•ä¸‹è½½
  python unified_crawler.py cninfo-excel links.xlsx --col 0     # ä» Excel é“¾æ¥åˆ—ä¸‹è½½
        '''
    )
    subparsers = parser.add_subparsers(dest='command', help='é€‰æ‹©æ•°æ®æº')

    # ---- sse-search ----
    p_search = subparsers.add_parser('sse-search', help='ä¸Šäº¤æ‰€å…¨ç«™æœç´¢çˆ¬è™«')
    p_search.add_argument('--step', choices=['all', 'crawl', 'merge', 'download'],
                          default='all', help='æ‰§è¡Œé˜¶æ®µ (é»˜è®¤: all)')
    p_search.add_argument('--keyword', required=True, help='æœç´¢å…³é”®è¯ï¼ˆå¿…å¡«ï¼‰')
    p_search.add_argument('-o', '--output', default=None, help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {SSE_SEARCH_OUTPUT_DIR})')

    # ---- sse-inquiry ----
    p_inquiry = subparsers.add_parser('sse-inquiry', help='ä¸Šäº¤æ‰€é—®è¯¢å‡½ä¸“æ çˆ¬è™«')
    p_inquiry.add_argument('--step',
                           choices=['crawl', 'test', 'download', 'verify', 'dedup', 'download-excel'],
                           default='crawl', help='æ‰§è¡Œæ­¥éª¤ (é»˜è®¤: crawl)')
    p_inquiry.add_argument('--json', default=None, help='æŒ‡å®š JSON æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº download/verifyï¼‰')
    p_inquiry.add_argument('-o', '--output', default=None, help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {SSE_INQUIRY_OUTPUT_DIR})')
    p_inquiry.add_argument('--col', type=int, default=0, help='Excel ä¸­é“¾æ¥æ‰€åœ¨åˆ—ç´¢å¼• (é»˜è®¤: 0)')

    # ---- cninfo ----
    p_cninfo = subparsers.add_parser('cninfo', help='å·¨æ½®èµ„è®¯ç½‘å…¬å‘Šä¸‹è½½å™¨')
    p_cninfo.add_argument('excel_file', help='Excel æ–‡ä»¶è·¯å¾„')
    p_cninfo.add_argument('-o', '--output', default=None, help=f'ä¿å­˜ç›®å½• (é»˜è®¤: {CNINFO_OUTPUT_DIR})')
    p_cninfo.add_argument('--start', type=int, default=0, help='èµ·å§‹ç´¢å¼• (é»˜è®¤: 0)')
    p_cninfo.add_argument('--end', type=int, default=None, help='ç»“æŸç´¢å¼• (é»˜è®¤: å…¨éƒ¨)')

    # ---- cninfo-search ----
    p_csearch = subparsers.add_parser('cninfo-search', help='å·¨æ½®å…³é”®è¯æ£€ç´¢ï¼ˆå»ºç´¢å¼• + ä¸‹è½½ï¼‰')
    p_csearch.add_argument('keyword', help='æ£€ç´¢å…³é”®è¯')
    p_csearch.add_argument('--step', choices=['index', 'download', 'all'],
                           default='index', help='æ‰§è¡Œæ­¥éª¤ (é»˜è®¤: index)')
    p_csearch.add_argument('-o', '--output', default=None, help=f'è¾“å‡ºç›®å½• (é»˜è®¤: {CNINFO_OUTPUT_DIR})')
    p_csearch.add_argument('--start-date', default=None, help='å¼€å§‹æ—¥æœŸ YYYY-MM-DD (é»˜è®¤: æœ€è¿‘30å¤©)')
    p_csearch.add_argument('--end-date', default=None, help='ç»“æŸæ—¥æœŸ YYYY-MM-DD (é»˜è®¤: ä»Šå¤©)')
    p_csearch.add_argument('--page-size', type=int, default=30, help='æ£€ç´¢é¡µå¤§å° (é»˜è®¤: 30, ä¸Šé™: 30)')
    p_csearch.add_argument('--max-pages', type=int, default=None, help='æœ€å¤§æŠ“å–é¡µæ•° (é»˜è®¤: ä¸é™åˆ¶)')
    p_csearch.add_argument('--max-results', type=int, default=None, help='æœ€å¤§è®°å½•æ•° (é»˜è®¤: ä¸é™åˆ¶)')
    p_csearch.add_argument('--index', default=None, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆdownload æ­¥éª¤ä½¿ç”¨ï¼‰')
    p_csearch.add_argument('--workers', type=int, default=MAX_DOWNLOAD_WORKERS,
                           help=f'ä¸‹è½½çº¿ç¨‹æ•° (é»˜è®¤: {MAX_DOWNLOAD_WORKERS})')

    # ---- cninfo-excel ----
    p_cexcel = subparsers.add_parser('cninfo-excel', help='ä» Excel é“¾æ¥åˆ—æ‰¹é‡ä¸‹è½½æ–‡ä»¶')
    p_cexcel.add_argument('excel_file', help='Excel æ–‡ä»¶è·¯å¾„')
    p_cexcel.add_argument('-o', '--output', default=None, help='è¾“å‡ºç›®å½•')
    p_cexcel.add_argument('--col', type=int, default=0, help='é“¾æ¥æ‰€åœ¨åˆ—ç´¢å¼• (é»˜è®¤: 0)')

    # ---- è§£æ ----
    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    print("=" * 60)
    print(f"  ç»Ÿä¸€çˆ¬è™«å·¥å…· | æ¨¡å—: {args.command}")
    print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    dispatch = {
        'sse-search': cmd_sse_search,
        'sse-inquiry': cmd_sse_inquiry,
        'cninfo': cmd_cninfo,
        'cninfo-search': cmd_cninfo_search,
        'cninfo-excel': cmd_cninfo_excel,
    }
    dispatch[args.command](args)


if __name__ == '__main__':
    main()
